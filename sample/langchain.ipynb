{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GOOGLE_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"gpt-3.5-turbo\": ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    \"claude-3-sonnet-20240229\": ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0),\n",
    "    \"gemini-pro\": ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "}\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Respond concisely.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chatbot chain\n",
    "def create_chain(model_name):\n",
    "    return (\n",
    "        RunnablePassthrough.assign(history=memory.load_memory_variables) | {\n",
    "            \"response\": prompt | models[model_name] | StrOutputParser()\n",
    "        }\n",
    "    ) | (lambda x: x[\"response\"])\n",
    "\n",
    "# Function to run the conversation\n",
    "def chat(user_input, model_name=\"anthropic\"):\n",
    "    chain = create_chain(model_name)\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": response})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'anthropic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow, let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms switch to OpenAI. Can you tell me about the Eiffel Tower?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(user_input, model_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(user_input, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input})\n\u001b[1;32m     20\u001b[0m     memory\u001b[38;5;241m.\u001b[39msave_context({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: response})\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mcreate_chain\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_chain\u001b[39m(model_name):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m         RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(history\u001b[38;5;241m=\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables) \u001b[38;5;241m|\u001b[39m {\n\u001b[0;32m---> 12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt \u001b[38;5;241m|\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     13\u001b[0m         }\n\u001b[1;32m     14\u001b[0m     ) \u001b[38;5;241m|\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'anthropic'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(chat(\"Hello, how are you?\"))\n",
    "print(chat(\"What's the capital of France?\"))\n",
    "print(chat(\"Now, let's switch to OpenAI. Can you tell me about the Eiffel Tower?\", \"openai\"))\n",
    "print(chat(\"Finally, let's use Gemini. What's the population of Tokyo?\", \"gemini\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. User Interface\n",
    "\n",
    "# In Jupyter Notebook, you can use widgets for a simple UI.\n",
    "from ipywidgets import Text, Button, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "def update_ui(user_input, model_name=\"anthropic\"):\n",
    "    response = chat(user_input, model_name)\n",
    "    display(Text(value=f\"You: {user_input}\"))\n",
    "    display(Text(value=f\"AI: {response}\"))\n",
    "\n",
    "# Create input and button widgets\n",
    "user_input = Text(description=\"Enter your message:\")\n",
    "send_button = Button(description=\"Send\")\n",
    "\n",
    "# Define behavior on button click\n",
    "send_button.on_click(lambda _: update_ui(user_input.value))\n",
    "\n",
    "# Display the UI\n",
    "display(VBox([user_input, send_button]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
